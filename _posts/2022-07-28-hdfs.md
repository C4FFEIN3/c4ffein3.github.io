---
layout: post
title: "single-node HDFS 설치 및 실행"
subtitle: "ubuntu 20.04에서 Hadoop Distributed File System (HDFS) single node로 설치 및 실행하기"
categories: hadoop-ecosystem
tags: [linux, ubuntu, hadoop-ecosystem, hdfs]
---

## java 설치

### openjdk-8-jdk 다운로드

```bash
sudo apt install openjdk-8-jdk
```

### 환경변수 설정

## 하둡 다운로드

### 다운로드 및 압축해제
- HDFS 3.2.3 버전으로 다운로드 하였음

```bash
wget https://downloads.apache.org/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz
tar -zxf hadoop-3.2.3.tar.gz
```

### 하둡 설정파일 설정

{hadoop의 root directory}/etc/hadoop에 존재

- **hadoop-env.sh**: JAVA_HOME 경로 설정

![Screenshot](https://user-images.githubusercontent.com/57282971/182780118-14038a90-0222-4bc2-8ee0-9c81a8bd71dc.png)

- **core-site.xml**: 클러스터의 네임노드에서 실행되는 하둡 데몬 설정
    
    하둡 파일 시스템 이름 설정 (URI 형식으로 입력)
    

```xml
<configuration>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://localhost:9000</value>
	</property>
</configuration>
```

- **hdfs-site.xml**: 네임노드와 데이터노드 저장 경로를 지정, 데이터 복제 개수를 설정

```xml
<configuration>
	<property>
		<name>dfs.replication</name>
		<value>1</value>
	</property>
	<property>
		<name>dfs.name.dir</name>
		<value>file:///home/noslab-ssd2/hadoop-3.2.3/hdfs/namenode</value>
	</property>
	<property>
		<name>dfs.data.dir</name>
		<value>file:///home/noslab-ssd2/hadoop-3.2.3/hdfs/datanode</value>
	</property>
</configuration>
```

- **mapred-site.xml**: 맵리듀스 설정
    
    기본 맵리듀스 프레임워크로 yarn을 설정
    

```xml
<configuration>
	<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>
	<property>
		<name>yarn.app.mapreduce.am.env</name>
		<value>HADOOP_MAPRED_HOME=/home/noslab-ssd2/hadoop</value>
	</property>
	<property>
		<name>mapreduce.map.env</name>
		<value>HADOOP_MAPRED_HOME=/home/noslab-ssd2/hadoop</value>
	</property>
	<property>
		<name>mapreduce.reduce.env</name>
		<value>HADOOP_MAPRED_HOME=/home/noslab-ssd2/hadoop</value>
	</property>
</configuration>
```

- **yarn-site.xml**: YARN 설정

```xml
<configuration>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
		<value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
</configuration>
```

## HDFS 네임노드 포맷

```bash
{hadoop의 root directory}/bin/hdfs namenode -format
```

## HDFS 실행

### DFS 실행

```bash
{hadoop의 root directory}/sbin/start-dfs.sh
```

### YARN 실행

```bash
{hadoop의 root directory}/sbin/start-yarn.sh
```

### 전체 실행

- DFS와 YARN을 한 번에 실행하기 위한 스크립트

```bash
{hadoop의 root directory}/sbin/start-all.sh
```

### 실행 확인

- **jps**: Java virtual machine Process Status tool. JVM에서 실행 중인 프로세스를 확인하기 위한 명령어

```bash
jps
```

### HDFS 종료

```bash
{hadoop의 root directory}/sbin/stop-all.sh
```

## HDFS Shell commands

### 명령어 기본 형태

- 명령어는 대부분 리눅스 shell command와 유사한 형태를 가지고 있음
- [https://hadoop.apache.org/docs/r3.2.3/hadoop-project-dist/hadoop-common/FileSystemShell.html](https://hadoop.apache.org/docs/r3.2.3/hadoop-project-dist/hadoop-common/FileSystemShell.html)

```bash
{하둡의 root directory}/bin/hadoop fs <args>
```

### ls

```bash
hadoop fs -ls {path}
```

### mkdir

```bash
hadoop fs -mkdir {new directory path}
```

### rm

- 파일 삭제

```bash
hadoop fs -rm {file path}
```

- 디렉토리 삭제

```bash
hadoop fs -rm -r {directory path}
```
