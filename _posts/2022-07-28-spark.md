---
layout: post
title: "Spark 설치 및 실행"
subtitle: "ubuntu 20.04에서 pyspark 설치 및 실행하기"
categories: hadoop-ecosystem
tags: [linux, ubuntu, hadoop-ecosystem, spark]
---

## pysark 설치

```bash
pip install pyspark
```

## pyspark shell 실행 예제

### 예제에 사용할 파일 (test.txt)

```
pyspark
pysparkpyspark
pysparkpysparkpyspark
pysparkpysparkpysparkpyspark
```

### pyspark shell 실행

```bash
pyspark
```

![Untitled](/assets/images/2022-07-28-spark/Untitled-00.png)

### 파일 읽기

```bash
>>> textFile = spark.read.text(”test.txt”)
```

### 텍스트 파일의 row 수 세기

```bash
>>> textFile.count()
4
```

### 텍스트 파일의 첫번째 row 보기

```bash
>>> textFile.first()
Row(value='pyspark')
```

### 특정 문자열이 포함된 row 수 세기

```bash
>>> textFile.filter(textFile.value.contains("Spark")).count()
0
>>> textFile.filter(textFile.value.contains("spark")).count()
4
```

## spark-submit 실행 예제

- **spark-submit**: 클러스터에서 어플리케이션을 실행하기 위해서 사용되는 스크립트

spark-submit을 이용해서 스파크를 사용하여 작성한 프로그램 코드를 실행시킬 수 있다.

### Spark를 통해 데이터를 파일에 쓰기 위한 코드 (writeSpark.py)

```python
from pyspark.sql import SparkSession

sparkSession = SparkSession.builder.appName("example-pyspark-write").getOrCreate()

data = [('First', 1), ('Second', 2), ('Third', 3), ('Fourth', 4), ('Fifth', 5)]
df = sparkSession.createDataFrame(data)

df.write.csv("example.csv")
```

- **SparkSession**의 **builder**를 이용해서 Spark와 상호작용하기 위한 세션을 생성
- **createdDataFrame**을 사용하여 sparkSession에 데이터프레임을 생성
- df**.write.csv(path)**를 사용하여 해당 데이터프레임을 지정된 경로에 csv 형태로 저장

### spark-submit을 이용하여 writeSpark.py 실행

```bash
spark-submit writeSpark.py
```

- ls를 통해 확인하면 example.csv가 생성된 것을 확인할 수 있음

![Untitled](/assets/images/2022-07-28-spark/Untitled-01.png)

- 생성된 example.csv는 단일 csv 파일이 아니라 Spark에서 분산처리를 위해 나눠 저장한 파일이 모여있는 디렉토리

![Untitled](/assets/images/2022-07-28-spark/Untitled-02.png)

![Untitled](/assets/images/2022-07-28-spark/Untitled-03.png)

### Spark를 통해 데이터를 파일에서 읽기 위한 코드 (readSpark.py)

- writeSpark.py에서 썼던 example.csv를 읽기 위한 코드

```python
from pyspark.sql import SparkSession

sparkSession = SparkSession.builder.appName("example-pyspark-read").getOrCreate()

df = sparkSession.read.csv("example.csv")
df.show()
```

### spark-submit을 이용하여 readSpark.py 실행

```bash
spark-submit readSpark.py
```

![Untitled](/assets/images/2022-07-28-spark/Untitled-04.png)
