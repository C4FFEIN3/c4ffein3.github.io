<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/jekyll-theme-yat/feed.xml" rel="self" type="application/atom+xml" /><link href="/jekyll-theme-yat/" rel="alternate" type="text/html" /><updated>2023-11-27T11:34:38+00:00</updated><id>/jekyll-theme-yat/feed.xml</id><title type="html">Delta</title><subtitle>Blog of Minseon Cho</subtitle><author><name>Minseon Cho(Delta)</name></author><entry><title type="html">NUMA (Non-Uniform Memory Architecture)</title><link href="/jekyll-theme-yat/linux-kernel/2023/11/27/numa.html" rel="alternate" type="text/html" title="NUMA (Non-Uniform Memory Architecture)" /><published>2023-11-27T00:00:00+00:00</published><updated>2023-11-27T00:00:00+00:00</updated><id>/jekyll-theme-yat/linux-kernel/2023/11/27/numa</id><content type="html" xml:base="/jekyll-theme-yat/linux-kernel/2023/11/27/numa.html"><![CDATA[<h2 id="시스템-토폴로지">시스템 토폴로지</h2>

<h3 id="cmp-chip-level-multi-processor">CMP (Chip-level Multi Processor)</h3>

<ul>
  <li>최근 CPU는 하나의 소켓에 멀티 코어로 구성 (하나의 칩에 여러 개의 프로세서)</li>
</ul>

<p><img src="https://github.com/C4FFEIN3/c4ffein3.github.io/assets/57282971/833af338-2184-4100-be35-4b08ce2b3c66" alt="cmp" /></p>

<h3 id="smp-symmetric-multi-processor--uma-uniform-memory-access">SMP (Symmetric Multi Processor) / UMA (Uniform Memory Access)</h3>

<ul>
  <li>멀티코어 CPU를 여러 개 장착한 시스템의 경우 메모리를 2개 이상의 CPU가 접근</li>
  <li>CPU와 메모리 사이를 네트워크로 연결하여 접근이 필요</li>
  <li>메모리를 공유하므로 한 번에 한 개의 프로세서만 하나의 메모리에 접근 가능 → 프로세서가 증가할수록 성능 문제 야기</li>
</ul>

<p><img src="https://github.com/C4FFEIN3/c4ffein3.github.io/assets/57282971/00529582-4c9f-4ab2-89db-47958348ca02" alt="smp" /></p>

<h3 id="numa-non-uniform-memory-access">NUMA (Non-Uniform Memory Access)</h3>

<ul>
  <li><strong>각 프로세서가 독립적인 로컬 메모리</strong>를 보유</li>
  <li>프로세서의 <strong>위치에 따라 메모리 접근 속도</strong>가 다름</li>
  <li>프로세서가 로컬 메모리에 접근할 때에는 다른 프로세서의 대기가 불필요하며 <strong>빠른 속도</strong>로 접근 가능</li>
  <li>각 CPU가 <strong>각 로컬 메모리에 접근하는 것은 동시</strong>에 일어날 수 있어서 성능 향상
    <ul>
      <li>로컬 메모리에 접근하는 것을 <strong>로컬 액세스(Local Access)</strong>라고 함</li>
    </ul>
  </li>
  <li>로컬 메모리가 아닌 <strong>다른 CPU에 붙어있는 메모리</strong>에 접근이 필요한 경우 <strong>메모리 접근 시간 증가</strong>
    <ul>
      <li>다른 노드의 메모리에 접근하는 것을 <strong>리모트 액세스(Remote Access)</strong>라고 함</li>
    </ul>
  </li>
  <li><strong>로컬 메모리에서 일어나는 메모리 접근 횟수</strong>가 성능 향상에 중요한 인자</li>
</ul>

<p><img src="https://github.com/C4FFEIN3/c4ffein3.github.io/assets/57282971/432e3472-814e-4717-85c0-765d6f737938" alt="numa" /></p>

<h2 id="numa-관련-커널bios-파라미터">NUMA 관련 커널/BIOS 파라미터</h2>

<h3 id="snc-sub-numa-clustering">SNC (Sub NUMA Clustering)</h3>

<ul>
  <li>1개의 NUMA 노드를 두 개의 NUMA 노드로 쪼개는 것
    <ul>
      <li>물리 CPU는 2개인데 시스템 상에서 인식하는 NUMA 노드는 4개</li>
    </ul>
  </li>
  <li>SNC가 켜지면 CPU의 리소스를 절반으로 나누어서 각각의 NUMA 도메인이 구성
    <ul>
      <li>LLC, 코어, 메모리 컨트롤러, 채널이 두 영역으로 나누어짐</li>
    </ul>
  </li>
  <li>이상적인 경우에 메모리 레이턴시가 감소하는 이점</li>
  <li>단, worst case에 대해서는 일반 NUMA보다 메모리 레이턴시가 증가
    <ul>
      <li>Node N번 LLC 확인
  → Node N번 컨트롤러/채널에서 확인 (best)
  → 같은 프로세서에 위치한 Node N+1번의 컨트롤러/채널에서 확인 (good)
  → 다른 프로세서에 위치한 노드의 컨트롤러/채널 확인 (worst)</li>
    </ul>
  </li>
  <li>BIOS 상에서 설정
    <ul>
      <li>일반적으로는 디폴트로 disable 설정</li>
    </ul>
  </li>
</ul>

<h3 id="bios의-node-interleaving">BIOS의 Node interleaving</h3>

<ul>
  <li>BIOS에서 NUMA를 기존의 SMP처럼 사용하도록 설정하는 기능을 제공</li>
  <li>BIOS 상에서 Node interleaving이라는 설정 값으로 존재
    <ul>
      <li>Node interleaving - disabled가 NUMA 사용</li>
      <li>Node interleaving - enabled가 NUMA 미사용</li>
    </ul>
  </li>
  <li>SMP 시스템처럼 전체 메모리에 일관된 주소로 접근 가능하지만 NUMA 구조를 SMP처럼 흉내내는 것이므로 SUMA(Sufficiently Uniform Memory Architecture)라고 불리기도 함</li>
  <li>어플리케이션이 NUMA와 맞지 않거나 최적화를 통한 빠른 성능보다는 균일한 응답속도와 대량의 메모리를 SMP처럼 사용하는 환경에 더 적합하다면 node interleaving을 활성화</li>
</ul>

<h3 id="kernel의-numa-balancing">Kernel의 NUMA balancing</h3>

<ul>
  <li>커널 레벨에서 NUMA 밸런싱을 제공하는 기능
    <ul>
      <li>주기적으로 프로세스의 메모리 매핑을 해제하고 메모리를 프로그램이 실행되는 노드로 옮기거나 태스크를 메모리에 가깝게 옮기는 기능 수행</li>
    </ul>
  </li>
  <li>커널 3.10부터 기본적으로 활성화 되어있음</li>
  <li>/proc/sys/kernel/numa_balancing 값으로 확인 가능
    <ul>
      <li>디폴트: 1 (활성화)</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>root@test:~# <span class="nb">cat</span> /proc/sys/kernel/numa_balancing
1
</code></pre></div></div>

<h3 id="kernel의-vmzone_reclaim_mode">Kernel의 vm.zone_reclaim_mode</h3>

<ul>
  <li>커널은 메모리를 사용 용도에 따라 zone이라 부르는 영역으로 구분하여 관리
    <ul>
      <li>zone 정보는 /proc/buddyinfo를 통해 확인 가능</li>
    </ul>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="o">[</span>root@test ~]# <span class="nb">cat</span> /proc/buddyinfo
  Node 0, zone      DMA      0      0      0      0      0      0      0      0      1      1      2
  Node 0, zone    DMA32      7     11      6      8      5      8      8      9      8      4    304
  Node 0, zone   Normal     66     53     14      4      1      3      2      3      1      2    189
  Node 1, zone   Normal     74     33      8      2      0      2      3      7     36     52   1172
</code></pre></div>    </div>
  </li>
  <li>DMA, DMA32는 Direct Memory Access를 위한 영역 (주로 오래된 하드웨어 동작을 위한 영역. 현재는 해당 영역을 필요로 하는 하드웨어 거의 없음)</li>
  <li>Normal은 일반적인 용도의 영역
    <ul>
      <li>커널, 프로세스 등이 메모리를 필요로 할 때 Normal 영역에서 메모리를 할당받아서 사용</li>
    </ul>
  </li>
  <li>vm.zone_reclaim_mode는 이 영역들 사이에서 특정 영역의 메모리가 부족할 경우 다른 영역의 메모리를 할당할 수 있게 함
    <ul>
      <li>4가지 값이 존재하나 실질적으로 0과 1이 중요</li>
      <li>0은 disable. zone 안에서 재할당하지 않음 → 다른 zone에서 가져와서 사용
        <ul>
          <li>page cache 등과 같은 재할당 대상 메모리들이 반환되지 않고 다른 노드에 있는 메모리를 할당 받아 사용</li>
          <li>메모리의 로컬 액세스로 인한 이득 &lt; 많은 양의 page cache를 확보함으로써 생기는 이득인 경우</li>
          <li>일반적으로는 0이 유리</li>
        </ul>
      </li>
      <li>1은 enable. zone 안에서 재할당 → 메모리 부족한 경우 해당 zone 안에서 재할당할 수 있는 메모리 영역을 먼저 찾아서 필요한 만큼 재할당하여 재사용. 그렇지 않은 경우 다른 zone에서 메모리 할당 받아 사용
        <ul>
          <li>메모리의 로컬 액세스로 인한 이득 &gt; page cache 확보로 인한 이득인 경우</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>root@test:~# <span class="nb">cat</span> /proc/sys/vm/zone_reclaim_mode
0
</code></pre></div></div>

<h2 id="리눅스-numa-cli-툴">리눅스 NUMA CLI 툴</h2>

<h3 id="numactl">numactl</h3>

<ul>
  <li>프로세스의 NUMA 사용 정보 확인 및 정책 설정 가능</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>root@test:~# numactl <span class="nt">-s</span>
policy: default
preferred node: current
physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31
cpubind: 0 1
nodebind: 0 1
membind: 0 1
root@test:~# numactl <span class="nt">-H</span>
available: 2 nodes <span class="o">(</span>0-1<span class="o">)</span>
node 0 cpus: 0 1 2 3 4 5 6 7 16 17 18 19 20 21 22 23
node 0 size: 15410 MB
node 0 free: 13070 MB
node 1 cpus: 8 9 10 11 12 13 14 15 24 25 26 27 28 29 30 31
node 1 size: 16083 MB
node 1 free: 15229 MB
node distances:
node   0   1
  0:  10  21
  1:  21  10
</code></pre></div></div>

<ul>
  <li><strong>available</strong> : NUMA 노드 개수 확인 가능</li>
  <li><strong>node # cpus/size/free</strong> : 각각의 노드에 해당하는 CPU 번호와 노드에 할당된 메모리 크기 확인 가능</li>
  <li><strong>node distances</strong> : 각 노드의 메모리에 접근하는데 걸리는 시간
    <ul>
      <li>0번 노드 → 1번 노드 메모리 접근에 걸리는 시간이 21이라는 뜻</li>
      <li>절대적인 시간은 아니고 상대적인 값</li>
      <li>0번 노드 → 0번 노드 메모리보다 2.1배 시간이 필요하다고 해석 가능.</li>
    </ul>
  </li>
</ul>

<h3 id="numastat">numastat</h3>

<ul>
  <li>numactl 패키지에 포함된 상태 확인 커맨드</li>
  <li>현재 NUMA 운영 상태 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@test ~]# numastat
                           node0           node1
numa_hit                17424515        12929757
numa_miss                      0         2128172
numa_foreign             2128172               0
interleave_hit           5273354         5273840
local_node               7187999        10706546
other_node              10236516         4351383
</code></pre></div></div>

<ul>
  <li>현재 NUMA 메모리 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@test ~]# numastat <span class="nt">-m</span>

Per-node system memory usage <span class="o">(</span><span class="k">in </span>MBs<span class="o">)</span>:
Token Node not <span class="k">in </span><span class="nb">hash </span>table.
Token Node not <span class="k">in </span><span class="nb">hash </span>table.
Token Node not <span class="k">in </span><span class="nb">hash </span>table.
Token Node not <span class="k">in </span><span class="nb">hash </span>table.
Token Node not <span class="k">in </span><span class="nb">hash </span>table.
Token Node not <span class="k">in </span><span class="nb">hash </span>table.
Token Node not <span class="k">in </span><span class="nb">hash </span>table.
Token Node not <span class="k">in </span><span class="nb">hash </span>table.
                          Node 0          Node 1           Total
                 <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
MemTotal                15346.38        16066.98        31413.36
MemFree                 12278.76        15093.87        27372.62
MemUsed                  3067.62          973.11         4040.74
Active                     28.92          351.93          380.85
Inactive                   62.17          195.05          257.21
Active<span class="o">(</span>anon<span class="o">)</span>                0.09            2.12            2.21
Inactive<span class="o">(</span>anon<span class="o">)</span>             11.14           49.48           60.62
Active<span class="o">(</span>file<span class="o">)</span>               28.83          349.81          378.64
Inactive<span class="o">(</span>file<span class="o">)</span>             51.03          145.56          196.59
Unevictable                 0.00            0.00            0.00
Mlocked                     0.00            0.00            0.00
Dirty                       0.00            0.00            0.00
Writeback                   0.00            0.00            0.00
FilePages                  81.21          511.11          592.33
Mapped                      0.29           55.20           55.49
AnonPages                  10.04           36.03           46.07
Shmem                       0.08           15.21           15.29
KernelStack                 4.13            3.32            7.45
PageTables                  0.93            4.00            4.93
NFS_Unstable                0.00            0.00            0.00
Bounce                      0.00            0.00            0.00
WritebackTmp                0.00            0.00            0.00
Slab                      369.82          244.15          613.96
SReclaimable               60.18           79.33          139.51
SUnreclaim                309.64          164.82          474.46
AnonHugePages               0.00            0.00            0.00
ShmemHugePages              0.00            0.00            0.00
ShmemPmdMapped              0.00            0.00            0.00
HugePages_Total             0.00            0.00            0.00
HugePages_Free              0.00            0.00            0.00
HugePages_Surp              0.00            0.00            0.00
</code></pre></div></div>

<ul>
  <li>특정 프로세스의 NUMA 메모리 사용 정보 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@test ~]# numastat <span class="nt">-p</span> bash

Per-node process memory usage <span class="o">(</span><span class="k">in </span>MBs<span class="o">)</span> <span class="k">for </span>PID 51870 <span class="o">(</span>bash<span class="o">)</span>
                           Node 0          Node 1           Total
                  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Huge                         0.00            0.00            0.00
Heap                         0.07            1.58            1.66
Stack                        0.01            0.09            0.11
Private                      0.02            3.57            3.59
<span class="nt">----------------</span>  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Total                        0.11            5.25            5.36
</code></pre></div></div>

<h3 id="numactl을-통한-프로세스-numa-정책-설정">numactl을 통한 프로세스 NUMA 정책 설정</h3>

<ul>
  <li><strong>–membind / -m</strong> : 지정된 노드에만 메모리 할당. 메모리 부족시 할당 실패</li>
  <li><strong>–cpunodebind / -N</strong> : 지정된 노드의 CPU에서만 프로세스 실행. 하나의 노드가 여러 CPU를 가질 수 있음</li>
  <li><strong>–physcpubind / -C</strong> : 지정된 CPU 번호에만 프로세스를 실행</li>
  <li><strong>–preferred / -p</strong> : 선호하는 node 지정. 리소스가 부족한 경우 다른 노드로부터 할당</li>
  <li><strong>–interleave / -i</strong> : 메모리를 RR로 지정된 노드들에 할당. all인 경우에 모든 노드들에 할당. 지정된 노드들로부터 메모리 할당에 실패하면 다른 노드들에 할당</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>root@test:~# numactl <span class="nt">--physcpubind</span><span class="o">=</span>0 <span class="nt">--</span> ./a.out
root@test:~# numactl <span class="nt">--preferred</span><span class="o">=</span>0 <span class="nt">--</span> ./a.out
root@test:~# numactl <span class="nt">--interleave</span><span class="o">=</span>all <span class="nt">--</span> ./a.out
</code></pre></div></div>

<h3 id="numad">numad</h3>

<ul>
  <li>현재 실행 중인 전체 프로세스에 대해서 리소스 사용 현황을 살피고 CPU, memory에 대한 affinity를 조절하는 툴</li>
  <li>각 프로세스가 동일한 노드의 CPU, 메모리를 사용하도록 조절
    <ul>
      <li>하나의 시스템에 수십, 수백개의 어플리케이션이 실행되거나 여러 가상 게스트 시스템이 동작하는 집약적 시스템 환경에 적합</li>
    </ul>
  </li>
  <li>numad -i 5는 numad가 시스템을 스캔하는 주기를 5초로 설정
    <ul>
      <li>numactl로 interleave 설정을 했음에도 RR로 고루 분산되는 것이 아니라 numad에 의해 한쪽 노드로 쏠리게 할당</li>
      <li>numad의 옵션 중 -K 옵션이 기본적으로 0으로 설정. -K 0은 interleave된 메모리를 로컬 메모리로 모으려고 함.</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@test ~]# numad <span class="nt">-i</span> 5
<span class="o">[</span>root@test ~]# numactl <span class="nt">--interleave</span><span class="o">=</span>all ./a.out &amp;
<span class="o">[</span>1] 51997
<span class="o">[</span>root@test ~]# numastat <span class="nt">-p</span> a.out

Per-node process memory usage <span class="o">(</span><span class="k">in </span>MBs<span class="o">)</span> <span class="k">for </span>PID 51997 <span class="o">(</span>a.out<span class="o">)</span>
                           Node 0          Node 1           Total
                  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Huge                         0.00            0.00            0.00
Heap                         0.00            0.00            0.00
Stack                        0.01            0.00            0.01
Private                   2320.49         2321.58         4642.07
<span class="nt">----------------</span>  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Total                     2320.50         2321.59         4642.09
<span class="o">[</span>root@test ~]# numastat <span class="nt">-p</span> a.out

Per-node process memory usage <span class="o">(</span><span class="k">in </span>MBs<span class="o">)</span> <span class="k">for </span>PID 51997 <span class="o">(</span>a.out<span class="o">)</span>
                           Node 0          Node 1           Total
                  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Huge                         0.00            0.00            0.00
Heap                         0.00            0.00            0.00
Stack                        0.01            0.00            0.01
Private                   2768.62         5362.45         8131.08
<span class="nt">----------------</span>  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Total                     2768.63         5362.46         8131.09
<span class="o">[</span>root@test ~]# numastat <span class="nt">-p</span> a.out

Per-node process memory usage <span class="o">(</span><span class="k">in </span>MBs<span class="o">)</span> <span class="k">for </span>PID 51997 <span class="o">(</span>a.out<span class="o">)</span>
                           Node 0          Node 1           Total
                  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Huge                         0.00            0.00            0.00
Heap                         0.00            0.00            0.00
Stack                        0.00            0.01            0.01
Private                   6898.71        13582.44        20481.15
<span class="nt">----------------</span>  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Total                     6898.71        13582.45        20481.17
<span class="o">[</span>root@test ~]# numastat <span class="nt">-p</span> a.out

Per-node process memory usage <span class="o">(</span><span class="k">in </span>MBs<span class="o">)</span> <span class="k">for </span>PID 51997 <span class="o">(</span>a.out<span class="o">)</span>
                           Node 0          Node 1           Total
                  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Huge                         0.00            0.00            0.00
Heap                         0.00            0.00            0.00
Stack                        0.00            0.01            0.01
Private                   6898.71        13582.44        20481.15
<span class="nt">----------------</span>  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Total                     6898.71        13582.45        20481.1
</code></pre></div></div>

<ul>
  <li>numad -K 1 옵션 사용시 interleave 된 메모리를 로컬 메모리로 모으지 않음</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@test ~]# numad <span class="nt">-K</span> 1 <span class="nt">-i</span> 5
<span class="o">[</span>root@test ~]# numactl <span class="nt">--interleave</span><span class="o">=</span>all ./a.out &amp;
<span class="o">[</span>1] 52071
<span class="o">[</span>root@test ~]# numastat <span class="nt">-p</span> a.out

Per-node process memory usage <span class="o">(</span><span class="k">in </span>MBs<span class="o">)</span> <span class="k">for </span>PID 52071 <span class="o">(</span>a.out<span class="o">)</span>
                           Node 0          Node 1           Total
                  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Huge                         0.00            0.00            0.00
Heap                         0.00            0.00            0.00
Stack                        0.01            0.00            0.01
Private                   2819.45         2820.42         5639.88
<span class="nt">----------------</span>  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Total                     2819.46         2820.43         5639.89
<span class="o">[</span>root@test ~]# numastat <span class="nt">-p</span> a.out

Per-node process memory usage <span class="o">(</span><span class="k">in </span>MBs<span class="o">)</span> <span class="k">for </span>PID 52071 <span class="o">(</span>a.out<span class="o">)</span>
                           Node 0          Node 1           Total
                  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Huge                         0.00            0.00            0.00
Heap                         0.00            0.00            0.00
Stack                        0.01            0.00            0.01
Private                   4388.39         4389.35         8777.74
<span class="nt">----------------</span>  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Total                     4388.39         4389.36         8777.75
<span class="o">[</span>root@test ~]# numastat <span class="nt">-p</span> a.out

Per-node process memory usage <span class="o">(</span><span class="k">in </span>MBs<span class="o">)</span> <span class="k">for </span>PID 52071 <span class="o">(</span>a.out<span class="o">)</span>
                           Node 0          Node 1           Total
                  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Huge                         0.00            0.00            0.00
Heap                         0.00            0.00            0.00
Stack                        0.01            0.00            0.01
Private                   8942.25         8943.22        17885.47
<span class="nt">----------------</span>  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Total                     8942.26         8943.23        17885.49
<span class="o">[</span>root@test ~]# numastat <span class="nt">-p</span> a.out

Per-node process memory usage <span class="o">(</span><span class="k">in </span>MBs<span class="o">)</span> <span class="k">for </span>PID 52071 <span class="o">(</span>a.out<span class="o">)</span>
                           Node 0          Node 1           Total
                  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Huge                         0.00            0.00            0.00
Heap                         0.00            0.00            0.00
Stack                        0.01            0.00            0.01
Private                  10240.04        10241.00        20481.04
<span class="nt">----------------</span>  <span class="nt">---------------</span> <span class="nt">---------------</span> <span class="nt">---------------</span>
Total                    10240.04        10241.01        20481.05
</code></pre></div></div>

<h2 id="참고자료">참고자료</h2>

<ul>
  <li>NUMA
    <ul>
      <li><a href="https://brunch.co.kr/@dreaminz/4">https://brunch.co.kr/@dreaminz/4</a></li>
      <li><a href="https://lunatine.net/2016/07/14/numa-with-linux/">https://lunatine.net/2016/07/14/numa-with-linux/</a></li>
    </ul>
  </li>
  <li>SNC
    <ul>
      <li><a href="https://byounghee.me/2022/09/22/numa-and-sub-numa-clustering/">https://byounghee.me/2022/09/22/numa-and-sub-numa-clustering/</a></li>
    </ul>
  </li>
</ul>]]></content><author><name>Minseon Cho(Delta)</name></author><category term="linux-kernel" /><category term="linux" /><category term="hardware" /><summary type="html"><![CDATA[시스템 토폴로지]]></summary></entry><entry><title type="html">[Linux command] chattr</title><link href="/jekyll-theme-yat/linux-kernel/2023/05/16/chattr.html" rel="alternate" type="text/html" title="[Linux command] chattr" /><published>2023-05-16T00:00:00+00:00</published><updated>2023-05-16T00:00:00+00:00</updated><id>/jekyll-theme-yat/linux-kernel/2023/05/16/chattr</id><content type="html" xml:base="/jekyll-theme-yat/linux-kernel/2023/05/16/chattr.html"><![CDATA[<h2 id="용도">용도</h2>
<ul>
  <li>디렉토리 혹은 파일의 속성 변경</li>
</ul>

<h2 id="사용방법">사용방법</h2>
<h3 id="설정">설정</h3>

<table>
  <thead>
    <tr>
      <th>옵션</th>
      <th>내용</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>+</td>
      <td>특정 속성 부여</td>
    </tr>
    <tr>
      <td>-</td>
      <td>특정 속성 제거</td>
    </tr>
  </tbody>
</table>

<h3 id="속성">속성</h3>

<table>
  <thead>
    <tr>
      <th>속성</th>
      <th>내용</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>a</td>
      <td>append만 가능</td>
    </tr>
    <tr>
      <td>c</td>
      <td>압축된 상태로 저장</td>
    </tr>
    <tr>
      <td>d</td>
      <td>dump로 백업 불가</td>
    </tr>
    <tr>
      <td>i</td>
      <td>read-only 상태</td>
    </tr>
    <tr>
      <td>s</td>
      <td>파일 삭제시 sync</td>
    </tr>
    <tr>
      <td>S</td>
      <td>파일 변경시 sync</td>
    </tr>
    <tr>
      <td>u</td>
      <td>삭제된 경우 그 내용이 저장되어 삭제되기 전 데이터로 복구 가능</td>
    </tr>
  </tbody>
</table>

<h3 id="예시">예시</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>chattr +i a.txt
</code></pre></div></div>
<ul>
  <li>a.txt 수정 불가</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>chattr <span class="nt">-i</span> a.txt
</code></pre></div></div>
<ul>
  <li>a.txt 수정 가능</li>
</ul>]]></content><author><name>Minseon Cho(Delta)</name></author><category term="linux-kernel" /><category term="linux" /><category term="command" /><summary type="html"><![CDATA[용도 디렉토리 혹은 파일의 속성 변경]]></summary></entry><entry><title type="html">[Linux command] mktemp</title><link href="/jekyll-theme-yat/linux-kernel/2023/05/16/mktemp.html" rel="alternate" type="text/html" title="[Linux command] mktemp" /><published>2023-05-16T00:00:00+00:00</published><updated>2023-05-16T00:00:00+00:00</updated><id>/jekyll-theme-yat/linux-kernel/2023/05/16/mktemp</id><content type="html" xml:base="/jekyll-theme-yat/linux-kernel/2023/05/16/mktemp.html"><![CDATA[<h2 id="용도">용도</h2>
<ul>
  <li>임시 파일 생성</li>
</ul>

<h2 id="사용방법">사용방법</h2>
<h3 id="옵션">옵션</h3>

<table>
  <thead>
    <tr>
      <th>옵션</th>
      <th>내용</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**-u**</td>
      <td>실제로 파일을 생성하지는 않으나 유니크한 파일 이름을 생성</td>
    </tr>
  </tbody>
</table>

<h3 id="예시">예시</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">mktemp</span>
/tmp/tmp.GrEUO8uPba
</code></pre></div></div>

<h4 id="특정-확장자를-가지는-임시-파일-생성">특정 확장자를 가지는 임시 파일 생성</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">mktemp</span> <span class="nt">--suffix</span> <span class="s2">".txt"</span>
/tmp/tmp.UwUkfdnLzm.txt
</code></pre></div></div>

<h4 id="특정-패턴을-가지는-임시-파일-생성">특정 패턴을 가지는 임시 파일 생성</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">mktemp </span>temp.XXXX
temp.X1fs
</code></pre></div></div>
<ul>
  <li>4개의 X가 임의의 글자로 치환되어 파일 생성</li>
</ul>

<h3 id="임시-디렉토리-생성">임시 디렉토리 생성</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">mktemp</span> <span class="nt">-d</span>
/tmp/tmp.MPS90T7lRV
</code></pre></div></div>]]></content><author><name>Minseon Cho(Delta)</name></author><category term="linux-kernel" /><category term="linux" /><category term="command" /><summary type="html"><![CDATA[용도 임시 파일 생성]]></summary></entry><entry><title type="html">Hard disk drive</title><link href="/jekyll-theme-yat/storage/2023/04/27/hdd.html" rel="alternate" type="text/html" title="Hard disk drive" /><published>2023-04-27T00:00:00+00:00</published><updated>2023-04-27T00:00:00+00:00</updated><id>/jekyll-theme-yat/storage/2023/04/27/hdd</id><content type="html" xml:base="/jekyll-theme-yat/storage/2023/04/27/hdd.html"><![CDATA[<h2 id="hard-disk-drive">Hard disk drive</h2>
<ul>
  <li><strong>자기적인(magnetic) 방식</strong>으로 데이터를 저장하는 보조기억장치</li>
</ul>

<h2 id="하드웨어-구조">하드웨어 구조</h2>

<p><img src="https://user-images.githubusercontent.com/57282971/228249117-244b954e-dd3f-4917-8852-a942636fffa2.png" alt="hdd" /></p>

<ul>
  <li><strong>스핀들 모터(Spindle motor)</strong>
    <ul>
      <li>플래터의 회전 담당</li>
    </ul>
  </li>
  <li><strong>액추에이터(Actuator)</strong>
    <ul>
      <li>액추에이터 암을 구동</li>
    </ul>
  </li>
  <li><strong>액추에이터 암(Actuator arm)</strong>
    <ul>
      <li>액추에이터를 통해 구동</li>
      <li>하나의 하드디스크에는 여러 개의 암이 존재</li>
      <li>플래터의 앞뒷면을 모두 사용하므로 한 플래터당 2개의 암 필요</li>
    </ul>
  </li>
  <li><strong>헤드(Head)</strong>
    <ul>
      <li>데이터를 읽고 쓰는 헤드</li>
    </ul>
  </li>
</ul>

<h3 id="데이터-접근-방식">데이터 접근 방식</h3>
<ul>
  <li>디스크가 회전하고 암이 앞뒤로 움직여 실제 <strong>데이터가 저장된 위치로 헤드를 물리적으로 이동</strong>시켜 데이터를 읽고 씀
→ <strong>순차적 접근(sequential write)</strong>이 성능적으로 유리</li>
  <li>턴테이블의 바늘이 레코드판과 접촉하는 것과 비슷해 보이지만 전혀 다른 방식
    <ul>
      <li>하드디스크의 <strong>헤드는 플래터와 접촉하지 않음</strong></li>
      <li><strong>전자기 유도 현상</strong>을 이용해 <strong>자기적인 방식</strong>으로 데이터를 읽고 씀
        <ul>
          <li>데이터를 읽는 경우, 헤드는 플래터 표면의 자기배열을 읽어들여서 데이터를 읽음</li>
          <li>데이터를 쓰는 경우, 헤드는 플래터 표면의 자기배열을 변경하여 데이터를 기록</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="디스크-구조">디스크 구조</h3>
<h4 id="플래터platter">플래터(Platter)</h4>
<ul>
  <li>데이터가 기록되는 판 하나</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/57282971/234841737-7d363cba-0ab9-4ae2-a395-79a436a9c0ef.png" alt="platter" /></p>

<h4 id="트랙track">트랙(Track)</h4>

<p><img src="https://user-images.githubusercontent.com/57282971/234841823-82d2055d-ff58-40cb-b9ba-75e82a98b20b.png" alt="track" /></p>

<h4 id="섹터sector">섹터(Sector)</h4>
<ul>
  <li>하드디스크의 데이터 접근 단위</li>
  <li>512 bytes</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/57282971/234841892-7a5d2f20-73cb-4f53-b45f-ad2abb4b3cc5.png" alt="sector" /></p>

<h4 id="실린더cylinder">실린더(Cylinder)</h4>

<p><img src="https://user-images.githubusercontent.com/57282971/234841942-a668ee9c-e17b-4a01-99d1-170bd12d1230.png" alt="cylinder" /></p>

<h2 id="섹터-주소-지정-방식">섹터 주소 지정 방식</h2>
<h3 id="chscylinder-head-sector">CHS(Cylinder-Head-Sector)</h3>
<ul>
  <li>초창기에 사용되던 섹터 주소 지정 방식으로 <strong>최근에는 잘 사용되지 않음</strong></li>
  <li><strong>Cylinder, Head, Sector의 위치</strong>를 토대로 주소를 지정</li>
  <li>인식할 수 있는 <strong>용량의 한계</strong></li>
</ul>

<h3 id="lbalogical-block-address">LBA(Logical Block Address)</h3>
<ul>
  <li>실제 섹터가 위치한 <strong>물리적 위치에 대해 고려하지 않음</strong></li>
  <li>모든 섹터가 연속하는 위치에 존재하는 것처럼 취급</li>
  <li><strong>0부터 순차적으로 주소 부여</strong>
    <ul>
      <li>첫번째 섹터가 LBA 0이 되고, 두번째 섹터가 LBA 1이 되는 식</li>
    </ul>
  </li>
</ul>

<h2 id="satasas">SATA/SAS</h2>
<ul>
  <li>하드디스크는 <strong>버스 인터페이스</strong>에 따라 주로 <strong>SATA HDD</strong>와 <strong>SAS HDD</strong>로 나뉨</li>
</ul>

<h3 id="sata-vs-sas">SATA vs SAS</h3>

<table>
  <thead>
    <tr>
      <th>|</th>
      <th>SATA (Serial ATA)</th>
      <th>SAS(Serial Attached SCSI)</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td> </td>
      <td>**가격**</td>
      <td>SAS에 비해 **저가**</td>
      <td>SATA에 비해 **고가**</td>
    </tr>
    <tr>
      <td> </td>
      <td>**속도**</td>
      <td>SAS에 비해 **느림** (SATA 3: 6000 Mbps)</td>
      <td>SATA에 비해 **빠름** (SAS 4: 22.5 Gbps)</td>
    </tr>
    <tr>
      <td> </td>
      <td>**호환성**</td>
      <td>SATA는 SAS 포트에 연결 가능</td>
      <td>SAS는 SATA 포트에 연결 불가능</td>
    </tr>
    <tr>
      <td> </td>
      <td>**핫플러그**</td>
      <td>지원</td>
      <td>지원</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>인터페이스의 전송 속도 차이는 크지만, 하드디스크의 물리적 전송 속도를 따지면 SATA 또한 물리적 전송 속도에 비해 빠른 편이므로 <strong>실질적인 속도 차이는 크지 않음</strong></li>
  <li>SAS가 SATA에 비해 안정성이 높았었으나, 현재 SATA의 발전으로 인해 SATA의 안정성 또한 크게 떨어지지 않음</li>
</ul>

<h2 id="smartctl">smartctl</h2>
<ul>
  <li>smartctl을 통해 디스크 상태를 확인 가능</li>
  <li>SATA/SAS 인터페이스 종류에 따라 smartctl 출력 결과가 다르므로 장애처리시 확인해야할 주요 요소의 이름이 다름</li>
</ul>]]></content><author><name>Minseon Cho(Delta)</name></author><category term="storage" /><category term="hardware" /><category term="fileIO" /><summary type="html"><![CDATA[Hard disk drive 자기적인(magnetic) 방식으로 데이터를 저장하는 보조기억장치]]></summary></entry><entry><title type="html">Multi-queue block I/O scheduler</title><link href="/jekyll-theme-yat/linux-kernel/2023/04/02/blkmq.html" rel="alternate" type="text/html" title="Multi-queue block I/O scheduler" /><published>2023-04-02T00:00:00+00:00</published><updated>2023-04-02T00:00:00+00:00</updated><id>/jekyll-theme-yat/linux-kernel/2023/04/02/blkmq</id><content type="html" xml:base="/jekyll-theme-yat/linux-kernel/2023/04/02/blkmq.html"><![CDATA[<h2 id="multi-queue-block-io-scheduler">Multi-queue block I/O scheduler</h2>
<ul>
  <li>blk-mq(Multi-queue block I/O queueing mechanism)의 I/O scheduler</li>
  <li>리눅스 커널 5.0부터 multi-queue block I/O scheduler를 기본 I/O scheduelr로 사용</li>
  <li>리눅스 커널 5.3부터 기존의 single queue block I/O scheduler를 지원하지 않음</li>
  <li>RHEL 8, Ubuntu 20부터 multi-queue block I/O scheduler를 기본 I/O scheduler로 사용</li>
</ul>

<h2 id="multi-queue-block-io-queueing">Multi-queue block I/O queueing</h2>
<h3 id="배경">배경</h3>
<ul>
  <li>기존의 디자인은 <strong>single queue와 하나의 lock</strong>으로 block I/O 요청 관리</li>
  <li>멀티 프로세서 환경에서 단일 lock을 이용한 I/O 관리는 <strong>병목 현상</strong> 야기 <br />
→ 이러한 문제를 해결하기 위해서 <strong>multi-queue block I/O 고안</strong></li>
  <li>blk-mq API에서는 <strong>각 CPU별로 개별적인 큐</strong>를 할당하여 lock으로 인한 성능 저하 문제를 해결</li>
</ul>

<h3 id="구조">구조</h3>
<p><img src="https://user-images.githubusercontent.com/57282971/229330942-c0c75059-4b2f-4702-adc9-bd6ac22a063b.png" alt="overview" /></p>

<ul>
  <li>blk-mq는 파일 시스템과 블록 I/O 디바이스 사이에서 동작</li>
  <li>blk-mq는 <strong>소프트웨어 큐</strong>(<strong>software staging queues</strong>)와 <strong>하드웨어 큐</strong>(<strong>hardware dispatch queues</strong>)의 두 종류의 큐들로 구성
    <ul>
      <li>I/O 스케줄러가 있거나 I/O 요청을 합쳐야(merge)할 필요가 있는 경우 I/O 요청은 소프트웨어 큐로 보내짐</li>
      <li>그 외의 경우 하드웨어 큐로 바로 보내짐</li>
      <li>소프트웨어 큐에서 I/O 요청이 처리된 이후 하드웨어 큐로 보내짐
        <ul>
          <li>하드웨어에 I/O 요청을 처리할 수 있는 충분한 리소스가 없는 경우, 처리 가능해질 때까지 임시 큐에 보관</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="소프트웨어-큐">소프트웨어 큐</h4>
<ul>
  <li>각각의 큐는 고유의 lock을 가지고 있고, 큐의 개수는 per-CPU나 per-node기반으로 결정</li>
  <li>Staging queue는 인접한 섹터에 해당하는 요청들을 합치는 역할
    <ul>
      <li>ex. 3-6, 6-7, 7-9의 요청이 있으면 3-9의 하나의 요청으로 합침</li>
    </ul>
  </li>
  <li>큐의 I/O 요청들은 <strong>I/O 스케줄러</strong>에 의해 재정렬될 수도 있음</li>
</ul>

<h4 id="하드웨어-큐">하드웨어 큐</h4>
<ul>
  <li>하드웨어 큐는 <strong>디바이스 드라이버의 디바이스 서브미션 큐(device submission queue) 혹은 디바이스 DMA ring buffer와 매핑</strong></li>
  <li>이 큐를 실행시키면 블록 레이어는 소프트웨어 큐에서 해당 요청들을 제거하고 하드웨어에 요청들을 디스패치</li>
  <li>하드웨어 큐의 개수는 하드웨어와 디바이스 드라이버가 지원하는 만큼 만들 수 있지만, 시스템의 코어 수를 넘지 않음</li>
  <li>재정렬 과정은 존재하지 않음 <strong>(I/O 스케줄러 사용하지 않음)</strong></li>
  <li>각 소프트웨어 큐에는 요청을 보낼 하드웨어 큐 집합이 있음</li>
</ul>

<h2 id="multi-queue-block-io-scheduler의-종류">Multi-queue block I/O scheduler의 종류</h2>
<h3 id="none">none</h3>

<blockquote>
  <p><strong>noop</strong></p>
  <ul>
    <li>single queue block I/O scheduler의 일종</li>
    <li>I/O 요청에 대해서 별도의 정렬 조치를 취하지 않는 방식. no operation.</li>
  </ul>
</blockquote>

<ul>
  <li>noop의 멀티 큐 버전</li>
  <li>I/O 요청에 대해서 별도의 정렬 조치를 취하지 않는 방식</li>
  <li>SSD나 고성능 스토리지 환경에서 CPU-bound 작업을 하는 경우에 적합</li>
  <li>RHEL 8, Ubuntu 20에서 multi-queue device의 default I/O scheduler</li>
</ul>

<h3 id="mq-deadline">mq-deadline</h3>
<blockquote>
  <p><strong>deadline</strong></p>
  <ul>
    <li>single queue block I/O scheduler의 일종</li>
    <li>I/O의 마감(deadline)을 지정하고, 마감이 가까운 I/O부터 처리</li>
    <li>지연 시간에 예민한 프로세스들에게 유리</li>
    <li>RHEL 7의 default I/O scheduler</li>
  </ul>
</blockquote>

<ul>
  <li>deadline의 멀티 큐 버전</li>
  <li>RHEL 8, Ubuntu 20에서 single queue device의 default I/O scheduler</li>
</ul>

<h4 id="configurable-parameter">Configurable parameter</h4>
<ul>
  <li>async_depth
    <ul>
      <li>default value: 1</li>
    </ul>
  </li>
  <li>fifo_batch
    <ul>
      <li>default value: 16</li>
      <li>배치에 속하는 요청의 최대값</li>
      <li>레이턴시와 처리량의 트레이드-오프
        <ul>
          <li>1으로 설정시 낮은 레이턴시</li>
          <li>높은 값은 높은 처리량</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>front_merges
    <ul>
      <li>default value: 1</li>
      <li>1은 enable, 0은 disable</li>
    </ul>
  </li>
  <li>write_expire
    <ul>
      <li>default value: 5000</li>
      <li>write의 데드라인 (ms 단위)</li>
    </ul>
  </li>
  <li>writes_starved
    <ul>
      <li>default value: 2</li>
      <li>얼마만큼 read를 write보다 우대하는지</li>
    </ul>
  </li>
</ul>

<h3 id="bfqbudget-fair-queueing">bfq(Budget Fair Queueing)</h3>
<blockquote>
  <p><strong>cfq(Completely Fair Queueing)</strong></p>
  <ul>
    <li>single queue block I/O scheduler의 일종</li>
    <li>모든 프로세스가 I/O를 균등하게 처리하는 방식</li>
    <li>각 프로세스가 I/O queue를 가지고, round robin 방식으로 정해진 time slice 내에 I/O 처리</li>
    <li>ubuntu 18의 default I/O scheduler</li>
  </ul>
</blockquote>

<ul>
  <li>각 프로세스마다 작업 큐를 가지고 있지만, cfq처럼 round robin 방식을 사용하지는 않음</li>
  <li>각 프로세스마다 I/O budget이 부여되고, 이 budget은 스토리지 접근 기회를 부여받았을 때 프로세스가 몇 개의 섹터만큼 데이터를 전송할 수 있는지를 나타냄</li>
  <li>데스크톱이나 interactive한 작업에 적합</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/57282971/229330961-17dd85cd-3fe5-42f4-adaf-b5e74dfc68bd.png" alt="bfq1" /></p>

<ul>
  <li>bfq의 budget 계산</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/57282971/229330966-fed44d6c-3261-43f6-8ff3-5b1286b98509.png" alt="bfq2" />
<img src="https://user-images.githubusercontent.com/57282971/229330977-4c75f694-97f8-453c-a0c1-3c94704105d3.png" alt="bfq3" /></p>

<h4 id="configurable-parameter-1">Configurable parameter</h4>
<ul>
  <li>back_seek_max
    <ul>
      <li>default value: 16384</li>
      <li>backward seeking의 최대값 (KB 단위)</li>
    </ul>
  </li>
  <li>back_seek_penalty
    <ul>
      <li>default value: 2</li>
      <li>backward seeking 계산에 사용되는 값</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><strong>Backward seek</strong> <br />
Disk head의 뒤에 위치한 블록을 탐색하는 것으로 bfq는 해당 탐색에 패널티를 부여</p>
</blockquote>

<ul>
  <li>fifo_expire_async
    <ul>
      <li>default value: 250</li>
      <li>비동기 요청의 타임아웃 (ms 단위)</li>
    </ul>
  </li>
  <li>fifo_expire_sync
    <ul>
      <li>default value: 125</li>
      <li>동기 요청의 타임아웃 (ms 단위)</li>
    </ul>
  </li>
  <li>low_latency
    <ul>
      <li>default value: 1</li>
      <li>1은 enable, 0은 disable</li>
    </ul>
  </li>
  <li>max_budget
    <ul>
      <li>default value: 0</li>
      <li>budget의 최대값</li>
      <li>0으로 설정된 경우 BFQ는 내부적으로 timeout_sync를 이용하여 최대값을 구함</li>
    </ul>
  </li>
  <li>slice_idle
    <ul>
      <li>default value: 8</li>
      <li>비어있는 큐에서 다음 요청을 위해서 기다릴 수 있는 최대 시간 (ms 단위)</li>
    </ul>
  </li>
  <li>slice_idle_us
    <ul>
      <li>default value: 8000</li>
      <li>slice_idle과 같으나 마이크로초 단위</li>
    </ul>
  </li>
  <li>strict_guarantees
    <ul>
      <li>default value: 0</li>
      <li>1은 enable, 0은 disable</li>
      <li>1로 설정되었을 경우 (a) 서비스 큐가 비어있으면 항상 대기하고, (b) 디바이스가 한 번에 하나의 I/O 요청을 처리하도록 강요</li>
    </ul>
  </li>
  <li>timeout_sync
    <ul>
      <li>default value: 125</li>
      <li>태스크(큐)가 선택된 후 서비스되는 최대 시간 (ms 단위)</li>
    </ul>
  </li>
</ul>

<h3 id="kyber">kyber</h3>
<ul>
  <li>빠른 멀티 큐 스토리지 장치를 대상으로 디자인</li>
  <li>SSD나 고성능 스토리지 환경에서 CPU-bound 작업을 하는 경우에 적합</li>
  <li>I/O 지연시간에 초점을 맞춘 스케줄러
    <ul>
      <li>블록 I/O 레이어에 submit 된 모든 I/O의 지연시간을 계산하여 지연시간에 대한 목표를 달성할 수 있도록 조정하는 스케줄러</li>
      <li>I/O 접근 요청의 도메인 별 지연시간 목표를 설정할 수 있음</li>
    </ul>
  </li>
  <li>도메인 별 설정</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/57282971/229331026-7f1873db-5aaa-4478-ac3d-da3e76d48d3b.png" alt="kyber" /></p>

<ul>
  <li>도메인 별 발생한 지연시간은 히스토그램으로 기록하여 계산에 사용</li>
</ul>

<h4 id="configurable-parameter-2">Configurable parameter</h4>
<ul>
  <li>read_lat_nsec
    <ul>
      <li>default value: 2000000</li>
      <li>read 지연시간 목표값</li>
    </ul>
  </li>
  <li>write_lat_nsec
    <ul>
      <li>default value: 10000000</li>
      <li>write 지연시간 목표값</li>
    </ul>
  </li>
</ul>

<h2 id="참고자료">참고자료</h2>
<ul>
  <li>[I/O schedulers] https://wiki.ubuntu.com/Kernel/Reference/IOSchedulers</li>
  <li>[bfq] https://lwn.net/Articles/601799/</li>
  <li>[kyber] https://lwn.net/Articles/720071/</li>
  <li>[kyber] https://www.phoronix.com/news/Linux-4.12-BFQ-Kyber</li>
</ul>]]></content><author><name>Minseon Cho(Delta)</name></author><category term="linux-kernel" /><category term="linux" /><category term="kernel" /><category term="blockIO" /><summary type="html"><![CDATA[Multi-queue block I/O scheduler blk-mq(Multi-queue block I/O queueing mechanism)의 I/O scheduler 리눅스 커널 5.0부터 multi-queue block I/O scheduler를 기본 I/O scheduelr로 사용 리눅스 커널 5.3부터 기존의 single queue block I/O scheduler를 지원하지 않음 RHEL 8, Ubuntu 20부터 multi-queue block I/O scheduler를 기본 I/O scheduler로 사용]]></summary></entry><entry><title type="html">Nand flash memory</title><link href="/jekyll-theme-yat/storage/2023/03/28/nandflash.html" rel="alternate" type="text/html" title="Nand flash memory" /><published>2023-03-28T00:00:00+00:00</published><updated>2023-03-28T00:00:00+00:00</updated><id>/jekyll-theme-yat/storage/2023/03/28/nandflash</id><content type="html" xml:base="/jekyll-theme-yat/storage/2023/03/28/nandflash.html"><![CDATA[<h2 id="nand-flash-memory">Nand flash memory</h2>
<ul>
  <li>전기적으로 작동하는 비휘발성 메모리로, 전기적으로 지우고 리프로그래밍 할 수 있음</li>
  <li>흔히 SATA SSD, NVMe SSD와 같은 고성능 스토리지
    <ul>
      <li>그 외에도 SD 카드, USB 플래시 드라이브 같은 것들이 존재</li>
    </ul>
  </li>
</ul>

<h2 id="하드웨어-구조">하드웨어 구조</h2>
<p><img src="https://user-images.githubusercontent.com/57282971/228248628-93749fad-3fff-49ce-a2a5-4b554d5d9a95.png" alt="ssd" /></p>

<p>SSD의 내부 사진 (출처: http://www.storagereview.com/samsung_ssd_840_pro_review)</p>

<ul>
  <li>PC의 CPU와 같은 역할을 하는 SSD 컨트롤러</li>
  <li>캐시 메모리 역할을 하는 DRAM</li>
  <li>데이터 저장을 하는 낸드 플래시 메모리
    <ul>
      <li>Die
        <ul>
          <li>낸드 플래시 구성요소의 가장 높은 레벨</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="https://user-images.githubusercontent.com/57282971/228248835-c552d9d6-3ed1-4313-8a74-0c9af138993c.png" alt="die" /></p>

<p><img src="https://user-images.githubusercontent.com/57282971/228248995-139a0844-ccbe-4ec1-a838-f0017d68452a.png" alt="block" /></p>

<ul>
  <li>읽기/쓰기 최소 단위인 페이지는 낸드 플래시 메모리마다 다르며 2KB, 4KB, 8KB, 16KB 중 하나</li>
  <li>block은 128 혹은 256 페이지로 구성</li>
</ul>

<h3 id="접근-방식">접근 방식</h3>

<table>
  <thead>
    <tr>
      <th>하드디스크</th>
      <th>플래시 메모리</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>자기 원반이 내부에 여러 개 들어 있고, 이 원반이 고속으로 **회전**하며 읽기, 쓰기 처리</td>
      <td>회로를 통한 **전기적인 접근**</td>
    </tr>
    <tr>
      <td>**회전 구조로 인해서 물리적인 접근 시간**이 필요하여 느림</td>
      <td>물리적인 접근 시간을 필요로 하는 하드디스크보다 빠르며, 랜덤 액세스에 유리한 것도 이러한 접근 방식 덕분</td>
    </tr>
  </tbody>
</table>

<h3 id="왜-nand-플래시-메모리인가">왜 “NAND” 플래시 메모리인가?</h3>
<ul>
  <li>NAND 게이트를 사용하므로 NAND 플래시 메모리</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/57282971/228249286-13cd59e8-565d-4c17-9d9a-39074d3b803d.png" alt="nandgate" /></p>

<h4 id="nor-플래시-메모리는-없는가">NOR 플래시 메모리는 없는가?</h4>
<ul>
  <li>NOR 플래시 메모리도 존재</li>
</ul>

<table>
  <thead>
    <tr>
      <th>|</th>
      <th>NAND 플래시 메모리</th>
      <th>NOR 플래시 메모리</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td> </td>
      <td>배열</td>
      <td>반도체 셀이 직렬로 배열</td>
      <td>반도체 셀이 병렬로 배열</td>
    </tr>
    <tr>
      <td> </td>
      <td>읽기 속도</td>
      <td>셀이 직렬로 배열되어 순차적으로 접근해야 하므로 **느림**</td>
      <td>셀이 병렬로 배열되어 **빠름**</td>
    </tr>
    <tr>
      <td> </td>
      <td>비트당 비용</td>
      <td>저가</td>
      <td>고가</td>
    </tr>
    <tr>
      <td> </td>
      <td>용량</td>
      <td>좁은 공간에 많은 셀을 집적할 수 있어 고용량화 가능</td>
      <td>집적이 어려워 고용량화가 힘들다</td>
    </tr>
    <tr>
      <td> </td>
      <td>쓰기 속도</td>
      <td>페이지 단위 쓰기를 하므로 **빠름**</td>
      <td>비트 단위 쓰기를 하므로 **느림**</td>
    </tr>
    <tr>
      <td> </td>
      <td>접근 단위</td>
      <td>페이지 단위 접근</td>
      <td>비트 단위 접근</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>고가에 저용량이므로 NOR 플래시 메모리는 잘 사용하지 않음</li>
</ul>

<h3 id="slcmlctlcqlc">SLC/MLC/TLC/QLC</h3>
<ul>
  <li>크기는 비슷한데 SSD의 용량이 늘어나는 건 어떤 원리일까?</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/57282971/228249466-ab3a9b7c-b9d4-4441-804b-1d9ede925b81.png" alt="slc" /></p>

<ul>
  <li>하나의 셀을 여러 개의 비트로 나눠서 쓸 수 있음
    <ul>
      <li>셀의 개수가 동일해도 용량 증가
        <ul>
          <li>비트 수가 작을수록 안정적이고 수명이 길고 빠름</li>
          <li>비트 수가 클수록 고용량</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="pe-cycle">P/E cycle</h3>
<h4 id="플래시-메모리의-쓰기programmed와-삭제erase">플래시 메모리의 쓰기(Programmed)와 삭제(Erase)</h4>

<p><img src="https://user-images.githubusercontent.com/57282971/228249592-e7956a19-eaf9-45b7-94ad-54686811ad8e.png" alt="pe" /></p>

<ul>
  <li>쓰기와 삭제를 반복하면 그 과정에서 통과해야 하는 <strong>oxide 층이 점차 마모되어 전자를 보관할 수 없는 순간</strong>이 옴
    <ul>
      <li>플래시 메모리에는 P/E(프로그램/삭제) 사이클이라고 불리는 <strong>수명</strong>이 존재</li>
    </ul>
  </li>
</ul>

<h2 id="읽기쓰기">읽기/쓰기</h2>
<h3 id="페이지-단위-읽기쓰기">페이지 단위 읽기/쓰기</h3>
<ul>
  <li>512 bytes 크기를 가진 섹터(sector) 단위로 접근했던 하드디스크와 달리 플래시 메모리는 페이지(page, 주로 4 KB) 단위로 접근</li>
</ul>

<h3 id="덮어쓰기">덮어쓰기</h3>
<ul>
  <li>플래시 메모리는 “free” 상태인 페이지에만 쓰기를 할 수 있음</li>
  <li>데이터가 변경되면 덮어 쓸 수 없음
    <ul>
      <li>데이터가 변경되면 페이지의 내용은 내부 레지스터로 복사된 후 레지스터에서 변경되어 새로운 “free” 상태의 페이지로 기록 (Read-Modify-Write)</li>
      <li>변경된 데이터가 새로운 페이지에 완전히 기록되면 원본 페이지는 “stale”(invalid)  상태가 되고 지워지기(erase) 전까지 그 상태로 존재한다</li>
      <li>지워지면 “free” 상태의 페이지가 됨</li>
    </ul>
  </li>
</ul>

<h3 id="삭제">삭제</h3>
<ul>
  <li>“stale” 상태의 페이지는 반드시 삭제(erase) 하여서 “free” 상태로 전환될 수 있음</li>
  <li>블록 단위로 실행
    <ul>
      <li>단일 페이지 단위로 처리 불가능</li>
      <li>페이지가 포함된 블록 전체를 삭제해야 함</li>
    </ul>
  </li>
  <li>사용자가 직접 사용할 수 있는 명령이 아니며 SSD 컨트롤러가 “free” 페이지 확보를 위해 Garbage collection을 일으킬 때 사용</li>
</ul>

<h3 id="wear-leveling">Wear leveling</h3>
<ul>
  <li>NAND 플래시 메모리는 프로그램-삭제(P/E cycles) 제한이 있으므로 제한된 수명을 가짐</li>
  <li>특정 블록에만 P/E가 빈번히 일어나면 해당 블록은 사용 불가능 상태가 됨 → 용량 감소</li>
  <li>SSD 컨트롤러에서는 이러한 상태를 막기 위하여 Wear leveling을 실행
    <ul>
      <li>전체 블록에 P/E가 골고루 분산되도록 하는 것</li>
      <li>목적은 모든 블록이 한번에 P/E 제한에 도달하여 한꺼번에 사용 불가능 상태가 되는 것</li>
      <li>특정 블록의 위치를 옮겨서 새로 써야할 수도 있으므로 write amplification 발생</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><strong>Write amplification</strong> <br />
실제 쓰고자 하는 데이터보다 더 많은 쓰기가 발생하는 것</p>
</blockquote>

<h3 id="garbage-collection">Garbage collection</h3>
<ul>
  <li>새로운 “free” 공간을 얻기 위하여 “stale” 상태의 페이지들을 정리하는 것</li>
  <li>GC를 수행하며 valid한 데이터를 옮기는 과정에서 write amplification 발생 → <strong>SSD 수명에 악영향</strong></li>
</ul>

<h4 id="gc-수행-방법">GC 수행 방법</h4>
<ol>
  <li>초기 상태</li>
</ol>

<p><img src="https://user-images.githubusercontent.com/57282971/228249715-07aed4fb-bf86-48f0-8dd3-9fd278c4b61a.png" alt="gc1" /></p>

<ol>
  <li>블록의 valid 데이터를 다른 블록에 복사</li>
</ol>

<p><img src="https://user-images.githubusercontent.com/57282971/228251389-d0eaf7d6-2892-4139-a98a-7106e91e4473.png" alt="gc2" /></p>

<ol>
  <li>복사를 마친 데이터 invalid 처리</li>
</ol>

<p><img src="https://user-images.githubusercontent.com/57282971/228251520-8dec4f1a-4f90-43ed-b779-5ddf0221c9ff.png" alt="gc3" /></p>

<ol>
  <li>비운 블록을 erase</li>
</ol>

<p><img src="https://user-images.githubusercontent.com/57282971/228251633-0df76235-d6c5-415d-991e-9073e8bbaaa5.png" alt="gc4" /></p>

<h4 id="핫콜드-데이터-분리">핫/콜드 데이터 분리</h4>
<ul>
  <li>핫 데이터: 빈번하게 변경되는 데이터</li>
  <li>콜드 데이터: 빈번하게 변경되지 않는 데이터</li>
  <li>동일 블록에 <strong>핫/콜드 데이터가 섞여있지 않는 것이 GC가 효율적</strong>으로 작동하는 데 도움이 됨
    <ul>
      <li>섞여있는 경우 위와 같이 콜드 데이터를 migration시켜줘야 하며 <strong>write amplification</strong> 증가</li>
    </ul>
  </li>
</ul>

<h4 id="랜덤-쓰기">랜덤 쓰기</h4>
<ul>
  <li>하드 디스크와 비교하면 매우 빠른 랜덤 쓰기 성능</li>
  <li>SSD는 랜덤 쓰기도 괜찮다? → <strong>괜찮지 않음!</strong>
    <ul>
      <li>하드 디스크보다 랜덤 쓰기 성능이 좋지만 그렇다고 해서 랜덤 쓰기가 SSD에 영향이 없는 건 아님</li>
      <li>누적될 경우 GC를 빈번하게 유발하는 원인이 되어서 수명과 성능에 악영향을 끼칠 수 있음</li>
    </ul>
  </li>
</ul>

<h4 id="gc에서-랜덤-쓰기의-영향">GC에서 랜덤 쓰기의 영향</h4>
<ul>
  <li>순차 쓰기 후 GC</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/57282971/228251737-0d3c74e3-a09b-4f34-98fb-3400215cd2f0.png" alt="seqgc" /></p>

<ul>
  <li>랜덤 쓰기 후 GC</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/57282971/228252139-c5ea3351-530d-41fd-9ff1-10ba141058dd.png" alt="randgc" /></p>

<ul>
  <li>랜덤 쓰기의 경우 GC 오버헤드가 훨씬 크다</li>
</ul>

<h3 id="over-provisioning">Over-provisioning</h3>
<ul>
  <li>SSD 용량은 왜 250GB, 500GB와 같이 2의 지수승이 아닌 것이 있는가? → over-provisioning 영역 때문</li>
  <li>일정 비율의 물리 블록을 SSD 컨트롤러는 볼 수 있지만 운영체제나 파일 시스템은 보지 못하도록 예약해두는 것
    <ul>
      <li>256GB SSD의 over-provisioning 영역이 16GB이면 240GB SSD, 6GB이면 250GB SSD, 0GB이면 256GB SSD</li>
      <li>하드웨어 스펙이 다른 것이 아니라 over-provisioning 영역의 크기로 인해 용량이 달라지는 것</li>
    </ul>
  </li>
</ul>

<h4 id="왜-op-영역이-필요한가">왜 OP 영역이 필요한가?</h4>
<ul>
  <li>SSD의 <strong>공간 사용률</strong>은 <strong>성능과 수명에 영향</strong>을 끼침
    <ul>
      <li>free 공간이 충분하면 비어있는 공간을 확보하기 위해 실행되는 GC가 잘 일어나지 않음
        <ul>
          <li>GC로 인한 write amplification 감소 → nand 수명 증가</li>
          <li>GC로 인한 성능 저하 발생 횟수 감소 → nand 성능 향상</li>
        </ul>
      </li>
      <li>GC가 발생하여도 충분한 free space로 간섭이 줄어들어 랜덤 쓰기 성능 향상</li>
      <li>over-provisioning을 통해 <strong>빈 공간을 확보</strong>하여 성능 저하를 막음</li>
    </ul>
  </li>
</ul>

<p><strong><em>→ OP 영역을 늘리면 nand의 랜덤 쓰기 성능 및 수명 증가</em></strong></p>

<ul>
  <li>SSD에는 프로그램-삭제(P/E cycles) 횟수 제한이 있음 → 제한을 초과하면 해당 셀은 쓸 수 없게 됨</li>
  <li><strong>쓸 수 없게 되는 셀</strong>이 발생하면 over-provisioning 영역을 이용해서 커버</li>
</ul>

<h3 id="trim">TRIM</h3>
<ul>
  <li>해당 데이터가 더 이상 사용되지 않는다는 것을 호스트에서 SSD 컨트롤러로 전달해주는 것</li>
  <li>NVMe에서는 “Deallocate”, SCSI/SAS에서는 “unmap”이라고 부름</li>
  <li>왜 필요한가?
    <ul>
      <li>SSD 컨트롤러는 삭제된 논리 블록의 주소를 알지 못함
        <ul>
          <li>파일 시스템에게서 덮어 쓰기 명령이 전달되어야 해당 공간이 비어있다는 사실을 알게 됨</li>
        </ul>
      </li>
      <li>이미 삭제된 블록이 valid 블록으로 보여서 GC 때 이동되며 불필요한 write ampliciation이 증가할 수 있음</li>
      <li>삭제된 블록을 알려줌으로서 불필요한 복사를 막을 수 있음</li>
      <li><strong>하지만 최근 SSD 내부 작동 방식이 많이 최적화되며 TRIM으로인한 성능 향상이 크지 않음</strong></li>
    </ul>
  </li>
  <li>SSD 컨트롤러, 운영체제, 파일 시스템이 모두 TRIM을 지원해야 사용 가능
    <ul>
      <li>리눅스 커널 2.6.33 이후 버전 지원</li>
      <li>ext4, XFS 등 지원</li>
    </ul>
  </li>
</ul>

<p><img src="https://user-images.githubusercontent.com/57282971/228252321-b444b2ed-2bdd-42d9-b205-ce3a9a17628b.png" alt="trim" /></p>

<h2 id="nvme-ssd">NVMe SSD</h2>
<h3 id="ssd-vs-nvme">SSD vs NVMe?</h3>
<ul>
  <li><strong>NVMe SSD도 SSD의 일종</strong>
    <ul>
      <li>SSD와 전혀 다른 하드웨어가 아님</li>
    </ul>
  </li>
  <li>낸드 플래시 메모리의 특성을 똑같이 가지고 있음
    <ul>
      <li><strong>물리적 구성 동일</strong></li>
    </ul>
  </li>
  <li><strong>인터페이스</strong> 타입이 달라 성능 차이가 발생
    <ul>
      <li>SATA SSD: <strong>ATA</strong> 기반의 SSD</li>
      <li>NVMe: <strong>PCIe</strong> 기반의 SSD</li>
      <li>컨트롤러, DRAM, 캐패시터 등 세부 기술에도 차이점이 존재하나 제일 큰 차이점은 인터페이스의 차이</li>
    </ul>
  </li>
  <li>NVMe SSD는 NVMe라는 전송 프로토콜 사용하여 더 향상된 성능을 보임
    <ul>
      <li>NVMe는 <strong>PCIe 버스</strong>를 통해 플래시 메모리에 접근</li>
      <li><strong>몇 만 개의 큐를 제공하여 병렬처리</strong>할 수 있도록 하여 단일 큐에 비해 빠른 속도</li>
    </ul>
  </li>
</ul>

<h4 id="pcie">PCIe</h4>
<ul>
  <li>고속 직렬 컴퓨터 확장 버스 표준</li>
  <li>여러 개의 레인을 가지고 있을 수 있음: x1, x2, x4…
    <ul>
      <li>x 다음에 오는 숫자만큼의 레인을 가지고 있는 것</li>
      <li>x1의 경우 레인이 1개이고, 데이터를 한 싸이클에 1비트를 전송</li>
      <li>x2의 경우 레인이 2개이고, 데이터를 한 싸이클에 2비트를 전송</li>
    </ul>
  </li>
</ul>

<h5 id="satapcie-처리량">SATA/PCIe 처리량</h5>

<table>
  <thead>
    <tr>
      <th>|</th>
      <th>x1</th>
      <th>x2</th>
      <th>x4</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td> </td>
      <td>SATA 1세대</td>
      <td>150MB/s</td>
      <td>.</td>
      <td>.</td>
    </tr>
    <tr>
      <td> </td>
      <td>SATA 2세대</td>
      <td>300MB/s</td>
      <td>.</td>
      <td>.</td>
    </tr>
    <tr>
      <td> </td>
      <td>SATA 3세대</td>
      <td>600MB/s</td>
      <td>.</td>
      <td>.</td>
    </tr>
    <tr>
      <td> </td>
      <td>PCIe 1세대</td>
      <td>250MB/s</td>
      <td>500MB/s</td>
      <td>1000MB/s</td>
    </tr>
    <tr>
      <td> </td>
      <td>PCIe 2세대</td>
      <td>500MB/s</td>
      <td>1000MB/s</td>
      <td>2000MB/s</td>
    </tr>
    <tr>
      <td> </td>
      <td>PCIe 3세대</td>
      <td>1GB/s</td>
      <td>2GB/s</td>
      <td>4GB/s</td>
    </tr>
    <tr>
      <td> </td>
      <td>PCIe 4세대</td>
      <td>2GB/s</td>
      <td>4GB/s</td>
      <td>8GB/s</td>
    </tr>
  </tbody>
</table>

<h2 id="참고자료">참고자료</h2>
<ul>
  <li><a href="https://tech.kakao.com/2016/07/13/coding-for-ssd-part-1/">개발자를 위한 SSD (카카오 테크 블로그)</a></li>
  <li><a href="https://www.flashmemorysummit.com/English/Collaterals/Proceedings/2011/20110810_T1A_Smith.pdf">SSD GC</a></li>
  <li><a href="https://www.usenix.org/conference/fast14/technical-sessions/presentation/jeong">[FAST’14] Lifetime Improvement of NAND Flash-based Storage Systems Using Dynamic Program and Erase Scaling</a></li>
</ul>]]></content><author><name>Minseon Cho(Delta)</name></author><category term="storage" /><category term="hardware" /><category term="fileIO" /><summary type="html"><![CDATA[Nand flash memory 전기적으로 작동하는 비휘발성 메모리로, 전기적으로 지우고 리프로그래밍 할 수 있음 흔히 SATA SSD, NVMe SSD와 같은 고성능 스토리지 그 외에도 SD 카드, USB 플래시 드라이브 같은 것들이 존재]]></summary></entry><entry><title type="html">FIO를 이용한 file I/O 성능 측정</title><link href="/jekyll-theme-yat/linux-kernel/2022/08/16/fio.html" rel="alternate" type="text/html" title="FIO를 이용한 file I/O 성능 측정" /><published>2022-08-16T00:00:00+00:00</published><updated>2022-08-16T00:00:00+00:00</updated><id>/jekyll-theme-yat/linux-kernel/2022/08/16/fio</id><content type="html" xml:base="/jekyll-theme-yat/linux-kernel/2022/08/16/fio.html"><![CDATA[<p><a href="https://github.com/axboe/fio">https://github.com/axboe/fio</a></p>

<h2 id="fio">FIO</h2>

<p>Flexible I/O tester</p>

<p>FIO를 이용해서 file I/O의 read/write IOPS와 같은 성능을 측정할 수 있다.</p>

<h2 id="fio-설치">FIO 설치</h2>

<h3 id="apt를-통한-설치">apt를 통한 설치</h3>

<p>간단하게 apt를 이용하여 설치할 수 있다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install </span>fio
</code></pre></div></div>

<h3 id="코드-컴파일을-통한-설치">코드 컴파일을 통한 설치</h3>

<p>FIO 코드의 커스텀이 필요하거나, 기본적으로 포함하지 않는 기능을 포함한 fio를 사용하고자 할 때에는 코드 컴파일을 통해 설치하여야 한다.</p>

<ul>
  <li><strong>아래의 링크에서 fio를 다운로드</strong></li>
</ul>

<p><a href="https://github.com/axboe/fio/releases">https://github.com/axboe/fio/releases</a></p>

<ul>
  <li><strong>압축 해제</strong></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#tar -xzf {다운로드 받은 fio 파일 이름}</span>
<span class="nb">tar</span> <span class="nt">-xzf</span> fio-3.30.tar.gz
</code></pre></div></div>

<ul>
  <li><strong>configuration 적용</strong></li>
</ul>

<p>configure에 option으로 기본적으로 포함되지 않는 기능을 추가할 수 있다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./configure
</code></pre></div></div>

<ul>
  <li><strong>빌드</strong></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make
</code></pre></div></div>

<ul>
  <li><strong>설치</strong></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make <span class="nb">install</span>
</code></pre></div></div>

<h2 id="fio-사용법">FIO 사용법</h2>

<p>fio는 (1)커맨드 라인에 option을 모두 설정하여 사용하는 방식과 (2)jobfile을 만들어 jobfile 내부에 설정을 기록해두고, 이를 실행하는 방식이 있다.</p>

<h3 id="command-line">Command line</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#fio [options]</span>
fio <span class="nt">--ioengine</span><span class="o">=</span>libaio <span class="nt">--direct</span><span class="o">=</span>1 <span class="nt">--rw</span><span class="o">=</span>randrw <span class="nt">--size</span><span class="o">=</span>1G
</code></pre></div></div>

<h3 id="jobfile">jobfile</h3>

<ul>
  <li>jobfile 생성</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vi job.fio
</code></pre></div></div>

<ul>
  <li>job.fio</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># -- job file 예시 --
#[global]
#rw=randread
#size=128m
#
#[job1]
#
#[job2]
#
# -- job file 예시 끝 --

[global]
ioengine=libaio
direct=1
rw=randrw
size=1G
</code></pre></div></div>

<ul>
  <li>jobfile 실행</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fio job.fio
</code></pre></div></div>

<h2 id="fio-options">FIO options</h2>

<p>수많은 옵션이 존재하므로 공식 문서를 확인하는 것을 추천</p>

<p><a href="https://fio.readthedocs.io/en/latest/fio_doc.html">https://fio.readthedocs.io/en/latest/fio_doc.html</a></p>]]></content><author><name>Minseon Cho(Delta)</name></author><category term="linux-kernel" /><category term="linux" /><category term="ubuntu" /><category term="kernel" /><category term="fileIO" /><category term="fio" /><summary type="html"><![CDATA[https://github.com/axboe/fio]]></summary></entry><entry><title type="html">FIO를 이용한 HDFS의 file I/O 성능 측정</title><link href="/jekyll-theme-yat/hadoop-ecosystem/2022/08/16/fio_hdfs.html" rel="alternate" type="text/html" title="FIO를 이용한 HDFS의 file I/O 성능 측정" /><published>2022-08-16T00:00:00+00:00</published><updated>2022-08-16T00:00:00+00:00</updated><id>/jekyll-theme-yat/hadoop-ecosystem/2022/08/16/fio_hdfs</id><content type="html" xml:base="/jekyll-theme-yat/hadoop-ecosystem/2022/08/16/fio_hdfs.html"><![CDATA[<h2 id="fio">FIO</h2>

<p><a href="https://c4ffein3.github.io/linux-kernel/2022/08/16/fio.html">FIO를 이용한 file I/O 성능 측정</a></p>

<h2 id="hdfs">HDFS</h2>

<p><a href="https://c4ffein3.github.io/hadoop-ecosystem/2022/07/28/hdfs.html">single-node HDFS 설치 및 실행</a></p>

<p><a href="https://c4ffein3.github.io/hadoop-ecosystem/2022/08/09/multinodeHDFS.html">multi-node HDFS 설치 및 실행</a></p>

<h2 id="hdfs-환경">HDFS 환경</h2>

<p>아래 게시글의 멀티노드 HDFS 클러스터 환경을 전제로 한다.</p>

<p><a href="https://c4ffein3.github.io/hadoop-ecosystem/2022/08/09/multinodeHDFS.html">multi-node HDFS 설치 및 실행</a></p>

<p><img src="https://user-images.githubusercontent.com/57282971/183603076-6fdfa7e0-5388-4db3-b1cb-c8be5b7cc1eb.png" alt="https://user-images.githubusercontent.com/57282971/183603076-6fdfa7e0-5388-4db3-b1cb-c8be5b7cc1eb.png" /></p>

<p>해당 게시글은 namenode에서 fio 실험을 하였기 때문에 로컬에 추가적인 HDFS 설치를 하지 않았으나, 별도의 컴퓨터에서 원격으로 fio를 실험하고자 하는 경우 로컬에 HDFS 설치가 필요하다. (fio 빌드 과정에서 HDFS library를 사용하므로)</p>

<h2 id="hdfs-실험을-위한-fio-설치">HDFS 실험을 위한 FIO 설치</h2>

<p>HDFS를 상대로 file I/O를 진행하는 것은 fio 기본 컴파일 옵션에 포함되어있지 않다. 그러므로 apt를 통해 fio를 설치하는 경우, HDFS 실험에는 사용할 수 없다.</p>

<h3 id="fio-소스코드-다운로드">FIO 소스코드 다운로드</h3>

<ul>
  <li><strong>아래의 링크에서 fio 다운로드</strong></li>
</ul>

<p><a href="https://github.com/axboe/fio/releases">https://github.com/axboe/fio/releases</a></p>

<ul>
  <li><strong>압축 해제</strong></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#tar -xzf {다운로드 받은 fio 파일 이름}</span>
<span class="nb">tar</span> <span class="nt">-xzf</span> fio-3.30.tar.gz
</code></pre></div></div>

<ul>
  <li><strong>libhdfs path 설정</strong></li>
</ul>

<p><strong>FIO_LIBHDFS_INCLUDE</strong>, <strong>FIO_LIBHDFS_LIB</strong>의 path를 설정해주어야 한다.</p>

<p>설정하지 않을시 configure를 실행하는 순간 다음과 같은 에러 메세지를 볼 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/57282971/184828434-01b0df51-c364-4acf-9309-069088c863bc.png" alt="Untitled" /></p>

<p>~/.bashrc에 path를 설정해준다.</p>

<p>path는 hdfs 디렉토리 내의 include와 library path를 설정</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vi ~/.bashrc
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#~/.bashrc 아래에 path 추가</span>
<span class="c">#export FIO_LIBHDFS_INCLUDE={HDFS 디렉토리 경로}/include</span>
<span class="c">#export FIO_LIBHDFS_LIB={HDFS 디렉토리 경로}/lib/native</span>
<span class="nb">export </span><span class="nv">FIO_LIBHDFS_INCLUDE</span><span class="o">=</span>/home/hadoop/hadoop-3.2.3/include
<span class="nb">export </span><span class="nv">FIO_LIBHDFS_LIB</span><span class="o">=</span>/home/hadoop/hadoop-3.2.3/lib/native
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source</span> ~/.bashrc
</code></pre></div></div>

<ul>
  <li><strong>configuration 적용</strong></li>
</ul>

<p><code class="language-plaintext highlighter-rouge">--enable-libf2fs</code> 옵션을 추가하여 HDFS 기능을 포함하도록 한다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./configure <span class="nt">--enable-libhdfs</span>
</code></pre></div></div>

<ul>
  <li>컴파일 및 설치</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make
make <span class="nb">install</span>
</code></pre></div></div>

<h2 id="hdfs-실험을-위한-fio-options">HDFS 실험을 위한 FIO options</h2>

<p>다른 I/O engine을 사용할 때와 대부분의 옵션을 동일하게 사용하지만, 몇 가지 옵션을 추가적으로 설정해줘야할 필요가 있다.</p>

<h3 id="ioenginelibhdfs">ioengine=libhdfs</h3>

<p>HDFS 실험을 할 경우 I/O engine은 libhdfs로 설정</p>

<h3 id="namenode">namenode</h3>

<p>HDFS 클러스터의 네임노드의 IP를 설정</p>

<h3 id="port">port</h3>

<p>HDFS 클러스터의 파일 시스템에 접근하기 위한 포트 설정</p>

<p>HDFS 설정시 fs.defaultFS에서 설정했던 포트를 입력한다.</p>

<h3 id="hdfsdirectory">hdfsdirectory</h3>

<p>HDFS 내부에서 I/O 실험을 실행할 디렉토리 설정</p>

<p>HDFS의 root 디렉토리의 경우 기본적으로 일반 유저의 접근 권한이 없으므로 이를 유의하여야 한다.</p>

<h3 id="예시">예시</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[global]
ioengine=libhdfs
namenode=10.100.54.177
port=9000
hdfsdirectory=/fio
chunk_size=128M

[job]
size=10G
blocksize=128M
rw=write
numjobs=1
</code></pre></div></div>]]></content><author><name>Minseon Cho(Delta)</name></author><category term="hadoop-ecosystem" /><category term="linux" /><category term="ubuntu" /><category term="hadoop-ecosystem" /><category term="fileIO" /><category term="fio" /><summary type="html"><![CDATA[FIO]]></summary></entry><entry><title type="html">multi-node HDFS 설치 및 실행</title><link href="/jekyll-theme-yat/hadoop-ecosystem/2022/08/09/multinodeHDFS.html" rel="alternate" type="text/html" title="multi-node HDFS 설치 및 실행" /><published>2022-08-09T00:00:00+00:00</published><updated>2022-08-09T00:00:00+00:00</updated><id>/jekyll-theme-yat/hadoop-ecosystem/2022/08/09/multinodeHDFS</id><content type="html" xml:base="/jekyll-theme-yat/hadoop-ecosystem/2022/08/09/multinodeHDFS.html"><![CDATA[<h2 id="overview">Overview</h2>

<p>구성하고자 하는 HDFS 구조</p>

<p><img src="https://user-images.githubusercontent.com/57282971/183603076-6fdfa7e0-5388-4db3-b1cb-c8be5b7cc1eb.png" alt="Untitled" /></p>

<h2 id="java-설치">java 설치</h2>

<h3 id="openjdk-8-jdk-다운로드">openjdk-8-jdk 다운로드</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>openjdk-8-jdk
</code></pre></div></div>

<h2 id="etchosts-수정">/etc/hosts 수정</h2>

<p>/etc/hosts는 IP와 hostname을 매핑하기 위한 파일</p>

<p>멀티 노드에 접속할 때의 편의를 위해 /etc/hosts 파일에 내용을 추가한다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>vi /etc/hosts
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/57282971/183603078-7c540526-734f-45b5-ba6c-c7ef88569379.png" alt="Untitled" /></p>

<p>아래와 같이 다른 노드의 IP와 hostname을 지정하는 내용을 추가한다.</p>

<p><img src="https://user-images.githubusercontent.com/57282971/183603084-1ae94810-b702-43a3-9cb5-b1dfd7284b59.png" alt="Untitled" /></p>

<h2 id="hadoop-사용자-생성">hadoop 사용자 생성</h2>

<p>멀티 노드로 사용하고자 하는 모든 노드들에서 하둡용 계정을 생성</p>

<p>하둡 설치 및 관련 설정은 모두 하둡용 계정에서 실행</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>adduser hadoop
</code></pre></div></div>

<p>계정에 로그인하기 위해서 아래 명령어를 사용한다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>su - hadoop
</code></pre></div></div>

<h2 id="ssh-key-설정">ssh key 설정</h2>

<p>기본적으로 하둡은 ssh를 이용하여 노드간 통신을 하므로, ssh 통신에 사용할 키를 생성해줘야 한다.</p>

<p>그리고 네임노드에서 데이터노드의 생성된 key를 이용해 접속할 수 있도록 키를 등록해줘야 한다.</p>

<ul>
  <li><strong>ssh key 생성</strong>: 모든 노드에서 실행</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh-keygen <span class="nt">-t</span> rsa
<span class="nb">cat</span> ~/.ssh/id_rsa.pub <span class="o">&gt;&gt;</span> ~/.ssh/authorized_key
</code></pre></div></div>

<p>실행시 ~/.ssh/ 폴더에 id_rsa.pub이라는 이름의 key가 생성된다.</p>

<ul>
  <li><strong>각 노드에서 생성된 ssh key 등록</strong>: 네임노드에서 실행</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh hadoop@noslab-ssd2 <span class="nb">cat</span> ~/.ssh/id_rsa.pub <span class="o">&gt;&gt;</span> ~/.ssh/authorized_keys
ssh hadoop@noslab-gpu <span class="nb">cat</span> ~/.ssh/id_rsa.pub <span class="o">&gt;&gt;</span> ~/.ssh/authorized_keys
ssh hadoop@noslab-gpu2 <span class="nb">cat</span> ~/.ssh/id_rsa.pub <span class="o">&gt;&gt;</span> ~/.ssh/authorized_keys
</code></pre></div></div>

<h2 id="하둡-파일-다운로드">하둡 파일 다운로드</h2>

<h3 id="하둡-파일-다운로드-및-압축해제">하둡 파일 다운로드 및 압축해제</h3>

<p>멀티 노드로 사용하고자 하는 모든 노드들에 하둡 설치</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>su - hadoop
wget https://downloads.apache.org/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz
<span class="nb">tar</span> <span class="nt">-zxf</span> hadoop-3.2.3.tar.gz
<span class="nb">cd </span>hadoop-3.2.3
</code></pre></div></div>

<h2 id="하둡-설정-파일-설정">하둡 설정 파일 설정</h2>

<p><strong>{hadoop의 root directory}/etc/hadoop</strong>에 존재</p>

<ul>
  <li><strong>hadoop-env.sh</strong>: JAVA_HOME 경로 설정</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/57282971/182780118-14038a90-0222-4bc2-8ee0-9c81a8bd71dc.png" alt="Screenshot from 2022-08-04 09-09-20.png" /></p>

<ul>
  <li>
    <p><strong>core-site.xml</strong>: 클러스터의 네임노드에서 실행되는 하둡 데몬 설정</p>

    <p>하둡 파일 시스템 이름 설정 (URI 형식으로 입력)</p>
  </li>
</ul>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;configuration&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>fs.defaultFS<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>hdfs://10.100.54.177:9000<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>hadoop.tmp.dir<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>file:///home/hadoop/hadoop-3.2.3/tmp<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div></div>

<p><em>fs.defaultFS</em>의 값은 <em>hdfs://{namenode의 IP}:9000</em>으로 지정</p>

<p>tmp 디렉터리의 경우 지정한 위치에 디렉터리를 생성해줄 필요</p>

<ul>
  <li><strong>hdfs-site.xml</strong>: 네임노드와 데이터노드 저장 경로를 지정, 데이터 복제 개수를 설정</li>
</ul>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;configuration&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>dfs.replication<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>3<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>dfs.namenode.name.dir<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>file:///home/hadoop/hadoop-3.2.3/namenode<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>dfs.datanode.data.dir<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>file:///home/hadoop/hadoop-3.2.3/datanode<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>dfs.http.address<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>10.100.54.177:50070<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div></div>

<p><em>dfs.replication</em>: 데이터 복제 개수</p>

<p><em>dfs.namenode.name.dir</em>: 네임노드의 디렉터리 경로</p>

<p><em>dfs.datanode.data.dir</em>: 데이터노드의 디렉터리 경로</p>

<p><em>dfs.http.address</em>: webUI를 통해서 dfs를 확인하고자 할 때의 address</p>

<ul>
  <li>
    <p><strong>mapred-site.xml</strong>: 맵리듀스 설정</p>

    <p>기본 맵리듀스 프레임워크로 yarn을 설정</p>
  </li>
</ul>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;configuration&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>mapreduce.framework.name<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>yarn<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div></div>

<ul>
  <li><strong>yarn-site.xml</strong>: YARN 설정</li>
</ul>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;configuration&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>mapreduce_shuffle<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>yarn.resourcemanager.hostname<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>10.100.54.177<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div></div>

<p><em>yarn.resourcemanager.hostname</em>의 값으로는 namenode의 IP를 입력해야함</p>

<ul>
  <li><strong>workers</strong>: datanode 설정</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>noslab-ssd2
noslab-gpu
noslab-gpu2
</code></pre></div></div>

<p>각 라인에 datanode의 hostname을 입력</p>

<ul>
  <li><strong>모든 파일 설정이 끝난 후에는 나머지 노드로 설정 파일을 복사</strong></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#위의 파일 수정을 namenode에서 했다고 가정했을 때</span>
scp ~/hadoop-3.2.3/etc/hadoop/<span class="k">*</span> hadoop@noslab-ssd2:~/hadoop-3.2.3/etc/hadoop/
scp ~/hadoop-3.2.3/etc/hadoop/<span class="k">*</span> hadoop@noslab-gpu:~/hadoop-3.2.3/etc/hadoop/
scp ~/hadoop-3.2.3/etc/hadoop/<span class="k">*</span> hadoop@noslab-gpu2:~/hadoop-3.2.3/etc/hadoop/
</code></pre></div></div>

<h2 id="bashrc-수정">.bashrc 수정</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vi ~/.bashrc
</code></pre></div></div>

<p>~/.bashrc를 열어 최하단에 아래 내용 추가</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">HADOOP_HOME</span><span class="o">=</span>/home/hadoop/hadoop-3.2.3
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:<span class="nv">$HADOOP_HOME</span>/bin
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:<span class="nv">$HADOOP_HOME</span>/sbin
<span class="nb">export </span><span class="nv">HADOOP_MAPRED_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">HADOOP_COMMON_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">HADOOP_HDFS_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">YARN_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
</code></pre></div></div>

<p>수정한 내용을 저장 후 적용</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source</span> ~/.bashrc
</code></pre></div></div>

<h2 id="hdfs-네임노드-포맷">HDFS 네임노드 포맷</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">{</span>hadoop의 root directory<span class="o">}</span>/bin/hdfs namenode <span class="nt">-format</span>
</code></pre></div></div>

<h2 id="hdfs-실행">HDFS 실행</h2>

<h3 id="dfs-실행">DFS 실행</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">{</span>hadoop의 root directory<span class="o">}</span>/sbin/start-dfs.sh
</code></pre></div></div>

<h3 id="yarn-실행">YARN 실행</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">{</span>hadoop의 root directory<span class="o">}</span>/sbin/start-yarn.sh
</code></pre></div></div>

<h3 id="전체-실행">전체 실행</h3>

<ul>
  <li>DFS와 YARN을 한 번에 실행하기 위한 스크립트</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">{</span>hadoop의 root directory<span class="o">}</span>/sbin/start-all.sh
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/57282971/183603087-1d7b9617-a5b2-4314-9e65-0267a623a81f.png" alt="Untitled" /></p>

<h3 id="실행-확인">실행 확인</h3>

<ul>
  <li><strong>jps</strong>: Java virtual machine Process Status tool. JVM에서 실행 중인 프로세스를 확인하기 위한 명령어</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jps
</code></pre></div></div>

<p>jps 실행시 namenode에 표시되는 프로세스:</p>

<p><img src="https://user-images.githubusercontent.com/57282971/183603093-e2367177-8592-4d88-a611-3134f88e74eb.png" alt="Untitled" /></p>

<p>jps 실행시 datanode에 표시되는 프로세스:</p>

<p><img src="https://user-images.githubusercontent.com/57282971/183603095-1b323732-9962-4af9-b70f-a5f978ef2d7f.png" alt="Untitled" /></p>

<ul>
  <li><strong>http://{namenode의 IP}:50070</strong>으로 접속할 경우 hdfs의 상태를 web UI로 확인 가능</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/57282971/183603100-24f85233-3cf8-46c5-8986-5b262215ef5f.png" alt="Untitled" /></p>

<p>Live Nodes의 수가 3개로 뜨는 것을 확인할 수 있다.</p>

<p>Configured Capacity의 경우 node들의 capacity 총합을 보여준다.</p>

<ul>
  <li><strong>http://{namenode의 IP}:8088</strong>으로 접속할 경우 yarn의 resource manager를 web UI로 확인 가능</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/57282971/183603101-3d6f0372-5df2-4661-bc80-4a5313fe727e.png" alt="Untitled" /></p>

<p>Nodes를 누르면 아래 화면을 볼 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/57282971/183603105-99a8b0f0-7f0b-4073-9ec1-ac804fbab0a0.png" alt="Untitled" /></p>

<p>설정한 노드들이 모두 정상적으로 작동하고 있는 것을 볼 수 있다.</p>

<h3 id="종료">종료</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">{</span>hadoop의 root directory<span class="o">}</span>/sbin/stop-all.sh
</code></pre></div></div>

<h2 id="발생했던-문제">발생했던 문제</h2>

<ul>
  <li>하둡 설정 파일에서 네임노드의 IP로 설정하는 대신 네임노드의 호스트네임으로 설정하는 경우(<em>e.g.</em> 10.100.54.177 대신 mscho-ubuntu 사용) 에러가 발생해서 데이터 노드와의 연결이 되지 않았음</li>
</ul>]]></content><author><name>Minseon Cho(Delta)</name></author><category term="hadoop-ecosystem" /><category term="linux" /><category term="ubuntu" /><category term="hadoop-ecosystem" /><category term="hdfs" /><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Spark에서 HDFS 파일 접근하기</title><link href="/jekyll-theme-yat/hadoop-ecosystem/2022/07/29/hadoop_spark.html" rel="alternate" type="text/html" title="Spark에서 HDFS 파일 접근하기" /><published>2022-07-29T00:00:00+00:00</published><updated>2022-07-29T00:00:00+00:00</updated><id>/jekyll-theme-yat/hadoop-ecosystem/2022/07/29/hadoop_spark</id><content type="html" xml:base="/jekyll-theme-yat/hadoop-ecosystem/2022/07/29/hadoop_spark.html"><![CDATA[<h2 id="하둡-실행">하둡 실행</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">{</span>hadoop의 root directory<span class="o">}</span>/sbin/start-all.sh
</code></pre></div></div>

<h2 id="spark를-통해-데이터를-hdfs에-쓰기">Spark를 통해 데이터를 HDFS에 쓰기</h2>

<h3 id="writesparkhdfspy">writeSparkHDFS.py</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">sparkSession</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">example-pyspark-write</span><span class="sh">"</span><span class="p">).</span><span class="nf">getOrCreate</span><span class="p">()</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="sh">'</span><span class="s">First</span><span class="sh">'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">Second</span><span class="sh">'</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">Third</span><span class="sh">'</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">Fourth</span><span class="sh">'</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">Fifth</span><span class="sh">'</span><span class="p">,</span> <span class="mi">5</span><span class="p">)]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">sparkSession</span><span class="p">.</span><span class="nf">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">df</span><span class="p">.</span><span class="n">write</span><span class="p">.</span><span class="nf">csv</span><span class="p">(</span><span class="sh">"</span><span class="s">hdfs://localhost:9000/example.csv</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><strong>HDFS의 path</strong>는 하둡의 core-site.xml 설정 파일에서 <strong>fs.defaultFS</strong>에 지정한 값
이 경우에는 hdfs://localhost:9000</li>
</ul>

<h3 id="spark-submit을-통해-writesparkhdfspy-실행">spark-submit을 통해 writeSparkHDFS.py 실행</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark-submit writeSparkHDFS.py
</code></pre></div></div>

<h3 id="hdfs-shell을-통해-생성된-파일-확인">HDFS shell을 통해 생성된 파일 확인</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hadoop fs <span class="nt">-ls</span> /
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/57282971/182781907-cc60efb8-382c-4234-9311-8071529842ce.png" alt="Untitled-00" /></p>

<h2 id="spark를-통해-데이터를-hdfs에서-읽기">Spark를 통해 데이터를 HDFS에서 읽기</h2>

<h3 id="readsparkhdfspy">readSparkHDFS.py</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">sparkSession</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">example-pyspark-read</span><span class="sh">"</span><span class="p">).</span><span class="nf">getOrCreate</span><span class="p">()</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">sparkSession</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nf">csv</span><span class="p">(</span><span class="sh">"</span><span class="s">hdfs://localhost:9000/user/example.csv</span><span class="sh">"</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="spark-submit을-통해-readsparkhdfspy-실행">spark-submit을 통해 readSparkHDFS.py 실행</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark-submit readSparkHDFS.py
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/57282971/182781912-d10b6f79-86b6-4cdc-8a33-a9814c175ddc.png" alt="Untitled-01" /></p>]]></content><author><name>Minseon Cho(Delta)</name></author><category term="hadoop-ecosystem" /><category term="linux" /><category term="ubuntu" /><category term="hadoop-ecosystem" /><category term="hdfs" /><category term="spark" /><summary type="html"><![CDATA[하둡 실행]]></summary></entry></feed>